{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vridhi-Wadhawan/stock-forecasting-mlops/blob/main/Stock_Forecasting_App_Deployment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# App Deployment"
      ],
      "metadata": {
        "id": "nmMWJtMi3f-p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Install Libraries ---\n",
        "# Install all required libraries including Streamlit, ML components, GenAI SDK, and pyngrok\n",
        "!pip install streamlit yfinance pandas numpy matplotlib xgboost scikit-learn joblib google-genai pyngrok\n",
        "\n",
        "import os\n",
        "import subprocess\n",
        "import time\n",
        "from pyngrok import ngrok\n",
        "from pyngrok.exception import PyngrokNgrokError\n",
        "import threading"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XBAAeOrXz8y1",
        "outputId": "aabf66c4-f97a-4d3f-cac8-423ba2d2be4b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting streamlit\n",
            "  Downloading streamlit-1.54.0-py3-none-any.whl.metadata (9.8 kB)\n",
            "Requirement already satisfied: yfinance in /usr/local/lib/python3.12/dist-packages (0.2.66)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.12/dist-packages (3.1.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (1.5.3)\n",
            "Requirement already satisfied: google-genai in /usr/local/lib/python3.12/dist-packages (1.61.0)\n",
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.5.0-py3-none-any.whl.metadata (8.1 kB)\n",
            "Requirement already satisfied: altair!=5.4.0,!=5.4.1,<7,>=4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (1.9.0)\n",
            "Collecting cachetools<7,>=5.5 (from streamlit)\n",
            "  Downloading cachetools-6.2.6-py3-none-any.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (8.3.1)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.12/dist-packages (from streamlit) (3.1.46)\n",
            "Requirement already satisfied: packaging>=20 in /usr/local/lib/python3.12/dist-packages (from streamlit) (26.0)\n",
            "Requirement already satisfied: pillow<13,>=7.1.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (11.3.0)\n",
            "Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
            "  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: protobuf<7,>=3.20 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.29.5)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (18.1.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.12/dist-packages (from streamlit) (2.32.4)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (9.1.2)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.12/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in /usr/local/lib/python3.12/dist-packages (from streamlit) (6.5.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (4.15.0)\n",
            "Requirement already satisfied: watchdog<7,>=2.1.5 in /usr/local/lib/python3.12/dist-packages (from streamlit) (6.0.0)\n",
            "Requirement already satisfied: multitasking>=0.0.7 in /usr/local/lib/python3.12/dist-packages (from yfinance) (0.0.12)\n",
            "Requirement already satisfied: platformdirs>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from yfinance) (4.5.1)\n",
            "Requirement already satisfied: pytz>=2022.5 in /usr/local/lib/python3.12/dist-packages (from yfinance) (2025.2)\n",
            "Requirement already satisfied: frozendict>=2.3.4 in /usr/local/lib/python3.12/dist-packages (from yfinance) (2.4.7)\n",
            "Requirement already satisfied: peewee>=3.16.2 in /usr/local/lib/python3.12/dist-packages (from yfinance) (3.19.0)\n",
            "Requirement already satisfied: beautifulsoup4>=4.11.1 in /usr/local/lib/python3.12/dist-packages (from yfinance) (4.13.5)\n",
            "Requirement already satisfied: curl_cffi>=0.7 in /usr/local/lib/python3.12/dist-packages (from yfinance) (0.14.0)\n",
            "Requirement already satisfied: websockets>=13.0 in /usr/local/lib/python3.12/dist-packages (from yfinance) (15.0.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.61.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.3.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.12/dist-packages (from xgboost) (2.27.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from xgboost) (1.16.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: anyio<5.0.0,>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from google-genai) (4.12.1)\n",
            "Requirement already satisfied: google-auth<3.0.0,>=2.47.0 in /usr/local/lib/python3.12/dist-packages (from google-auth[requests]<3.0.0,>=2.47.0->google-genai) (2.47.0)\n",
            "Requirement already satisfied: httpx<1.0.0,>=0.28.1 in /usr/local/lib/python3.12/dist-packages (from google-genai) (0.28.1)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.9.0 in /usr/local/lib/python3.12/dist-packages (from google-genai) (2.12.3)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from google-genai) (1.9.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from google-genai) (1.3.1)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.12/dist-packages (from pyngrok) (6.0.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (3.1.6)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (4.26.0)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (2.16.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5.0.0,>=4.8.0->google-genai) (3.11)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4>=4.11.1->yfinance) (2.8.3)\n",
            "Requirement already satisfied: cffi>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from curl_cffi>=0.7->yfinance) (2.0.0)\n",
            "Requirement already satisfied: certifi>=2024.2.2 in /usr/local/lib/python3.12/dist-packages (from curl_cffi>=0.7->yfinance) (2026.1.4)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0.0,>=2.47.0->google-auth[requests]<3.0.0,>=2.47.0->google-genai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0.0,>=2.47.0->google-auth[requests]<3.0.0,>=2.47.0->google-genai) (4.9.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0,>=0.28.1->google-genai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0.0,>=0.28.1->google-genai) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.9.0->google-genai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.9.0->google-genai) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.9.0->google-genai) (0.4.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (2.5.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.12.0->curl_cffi>=0.7->yfinance) (3.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (3.0.3)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (25.4.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (0.37.0)\n",
            "Requirement already satisfied: rpds-py>=0.25.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (0.30.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0,>=2.47.0->google-auth[requests]<3.0.0,>=2.47.0->google-genai) (0.6.2)\n",
            "Downloading streamlit-1.54.0-py3-none-any.whl (9.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.1/9.1 MB\u001b[0m \u001b[31m89.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyngrok-7.5.0-py3-none-any.whl (24 kB)\n",
            "Downloading cachetools-6.2.6-py3-none-any.whl (11 kB)\n",
            "Downloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m117.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyngrok, cachetools, pydeck, streamlit\n",
            "  Attempting uninstall: cachetools\n",
            "    Found existing installation: cachetools 7.0.0\n",
            "    Uninstalling cachetools-7.0.0:\n",
            "      Successfully uninstalled cachetools-7.0.0\n",
            "Successfully installed cachetools-6.2.6 pydeck-0.9.1 pyngrok-7.5.0 streamlit-1.54.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Setting Enviroment Variables  ---\n",
        "os.environ[\"GEMINI_API_KEY\"] = \"AIzaSyB7wM_txavE_2xLsHQvs7fqwwKfO2iiDko\"\n",
        "os.environ[\"NGROK_AUTH_TOKEN\"] = \"360zPaErQ3n0rJnh34kIqFHFF3N_3PuofpKeMEuGZYLe8NJpd\"\n",
        "print(\"API and NGROK keys set in current environment.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WA2hE-uw0Iig",
        "outputId": "6be42488-a8ec-4b1f-8f87-c77fbe1628b8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "API and NGROK keys set in current environment.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0cd5387c",
        "outputId": "88621996-2544-40d5-9659-00aa4448bf9c"
      },
      "source": [
        "%%writefile requirements.txt\n",
        "streamlit\n",
        "yfinance\n",
        "pandas\n",
        "numpy\n",
        "matplotlib\n",
        "xgboost\n",
        "scikit-learn\n",
        "joblib\n",
        "google-generativeai\n",
        "pyngrok\n",
        "seaborn"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing requirements.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6dbe29e5",
        "outputId": "8b4d835b-340d-4bfc-e542-662a0cdd47ba"
      },
      "source": [
        "print('requirements.txt generated successfully.')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "requirements.txt generated successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Saving the app.py file ---\n",
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import yfinance as yf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime, timedelta\n",
        "from google import genai\n",
        "import os\n",
        "import threading\n",
        "import time\n",
        "import subprocess\n",
        "\n",
        "# Set Plot Style for better visuals\n",
        "plt.style.use('ggplot')\n",
        "sns.set_style('whitegrid')\n",
        "\n",
        "# --- GLOBAL CONFIGURATION ---\n",
        "FEATURE_COLS = [\n",
        "    'Log_Returns', 'lag_1', 'lag_2', 'lag_5', 'lag_10', 'lag_21',\n",
        "    'MA_5', 'MA_10', 'MA_21', 'VOL_5', 'VOL_10', 'VOL_21',\n",
        "    'RSI_14', 'EMA_10', 'EMA_20'\n",
        "]\n",
        "\n",
        "# --- Static Model Performance & Historical Data ---\n",
        "MODEL_METRICS = {\n",
        "    # Using the R2 scores you provided in the previous message\n",
        "    '1D_R2': 0.72,\n",
        "    '30D_R2': 0.65,\n",
        "    'Retrain_Threshold': 0.60}\n",
        "\n",
        "@st.cache_data\n",
        "def load_historical_predictions():\n",
        "    \"\"\"Loads historical model performance data (Actual vs. Predicted) for plotting.\"\"\"\n",
        "    try:\n",
        "        # Using index_col=0 to correctly read the index column\n",
        "        df_hist = pd.read_csv(\"historical_predictions.csv\", index_col=0, parse_dates=True)\n",
        "        return df_hist\n",
        "    except FileNotFoundError:\n",
        "        st.warning(\"Historical predictions file (historical_predictions.csv) not found. Past performance chart skipped.\")\n",
        "        return None\n",
        "\n",
        "HISTORICAL_PREDICTIONS = load_historical_predictions()\n",
        "\n",
        "# --- Load Models & Scaler ---\n",
        "@st.cache_resource\n",
        "def load_resources():\n",
        "    try:\n",
        "        xgb_1d = pickle.load(open(\"xgb_1d.pkl\", \"rb\"))\n",
        "        xgb_30d = pickle.load(open(\"xgb_30d.pkl\", \"rb\"))\n",
        "        scaler = pickle.load(open(\"scaler.pkl\", \"rb\"))\n",
        "\n",
        "        # --- Extract Feature Importance ---\n",
        "        importance_scores = xgb_30d.feature_importances_\n",
        "        feature_importance = dict(zip(FEATURE_COLS, importance_scores))\n",
        "\n",
        "        return xgb_1d, xgb_30d, scaler, feature_importance\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        st.error(\"Model files (xgb_1d.pkl, xgb_30d.pkl) or scaler (scaler.pkl) not found. Please upload them.\")\n",
        "        return None, None, None, None\n",
        "\n",
        "xgb_1d, xgb_30d, scaler, FEATURE_IMPORTANCE = load_resources()\n",
        "\n",
        "# --- Preprocessing Function ---\n",
        "def preprocess_new_data(df, scaler):\n",
        "    \"\"\"\n",
        "    Preprocess raw Yahoo stock data and return the last row of ML-ready features, price, and the DataFrame.\n",
        "    \"\"\"\n",
        "    df = df.copy()\n",
        "    df = df.sort_index()\n",
        "\n",
        "    # Log Returns\n",
        "    df['Log_Returns'] = np.log(df['Adj Close'] / df['Adj Close'].shift(1))\n",
        "\n",
        "    # Lags\n",
        "    for lag in [1, 2, 5, 10, 21]:\n",
        "        df[f'lag_{lag}'] = df['Log_Returns'].shift(lag)\n",
        "\n",
        "    # Moving Averages (Price & Volume)\n",
        "    for window in [5, 10, 21]:\n",
        "        df[f'MA_{window}'] = df['Adj Close'].rolling(window).mean()\n",
        "        df[f'VOL_{window}'] = df['Volume'].rolling(window).mean()\n",
        "\n",
        "    # RSI (14)\n",
        "    def calculate_rsi(series, window=14):\n",
        "        delta = series.diff()\n",
        "        gain = delta.where(delta > 0, 0)\n",
        "        loss = -delta.where(delta < 0, 0)\n",
        "        avg_gain = gain.ewm(com=window-1, min_periods=window).mean()\n",
        "        avg_loss = loss.ewm(com=window-1, min_periods=window).mean()\n",
        "        # Handle division by zero for initial periods (where avg_loss might be 0)\n",
        "        rs = avg_gain.divide(avg_loss.replace(0, np.nan)).fillna(0)\n",
        "        return 100 - (100 / (1 + rs))\n",
        "\n",
        "    df['RSI_14'] = calculate_rsi(df['Adj Close'])\n",
        "\n",
        "    # EMA\n",
        "    df['EMA_10'] = df['Adj Close'].ewm(span=10, adjust=False).mean()\n",
        "    df['EMA_20'] = df['Adj Close'].ewm(span=20, adjust=False).mean()\n",
        "\n",
        "    # Clean NaNs\n",
        "    df = df.dropna()\n",
        "\n",
        "    if df.empty:\n",
        "        return None, None, None\n",
        "\n",
        "    X_all = df[FEATURE_COLS]\n",
        "\n",
        "    # Scale features\n",
        "    try:\n",
        "        X_scaled_all = scaler.transform(X_all)\n",
        "    except Exception as e:\n",
        "        st.error(f\"Scaling error: {e}. Ensure features match those used in training.\")\n",
        "        return None, None, None\n",
        "\n",
        "    # Get last row for prediction\n",
        "    last_features_scaled = X_scaled_all[-1].reshape(1, -1)\n",
        "    last_price = df['Adj Close'].iloc[-1].item()\n",
        "\n",
        "    return last_features_scaled, last_price, df\n",
        "\n",
        "# --- AI Advice Function (LLM-Powered Narrative) ---\n",
        "def generate_llm_insight(ticker, last_price, pred_price_1d, pred_price_30d, change_1d, change_30d, feature_importance, r2_score):\n",
        "    \"\"\"\n",
        "    Calls the LLM to generate a narrative summary based on all model outputs.\n",
        "    \"\"\"\n",
        "\n",
        "    # 1. Prepare the data payload for the LLM\n",
        "    data_summary = {\n",
        "        \"Ticker\": ticker,\n",
        "        \"Last_Price\": f\"₹{last_price:.2f}\",\n",
        "        \"Predicted_1D_Price\": f\"₹{pred_price_1d:.2f} ({change_1d:.2f}%)\",\n",
        "        \"Predicted_30D_Price\": f\"₹{pred_price_30d:.2f} ({change_30d:.2f}%)\",\n",
        "        \"Model_R2_Score\": f\"{r2_score:.2f}\",\n",
        "        \"Top_Feature_Importance\": dict(sorted(feature_importance.items(), key=lambda item: item[1], reverse=True)[:3])\n",
        "    }\n",
        "\n",
        "    # 2. Construct the system prompt\n",
        "    prompt = f\"\"\"\n",
        "    You are a highly experienced Financial Analyst providing a daily market commentary.\n",
        "    Your task is to analyze the provided stock forecast data for **{ticker}** and write a concise, professional, and actionable insight summary in 5-7 sentences.\n",
        "\n",
        "    Follow these rules:\n",
        "    1.  **Start** with a clear verdict.\n",
        "    2.  **Highlight** the most important factor driving the 1-day and the 30-day prediction (use the 'Top_Feature_Importance' data).\n",
        "    3.  **Include** a risk or caution note based on the Model_R2_Score (If R2 is < 0.70, mention it).\n",
        "    4.  **Do NOT** include any introductory or concluding phrases like 'Based on the data' or 'In conclusion'. Just provide the commentary.\n",
        "    5.  Use a professional and objective tone.\n",
        "\n",
        "    Data for Analysis (JSON): {data_summary}\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        api_key = os.environ.get(\"GEMINI_API_KEY\")\n",
        "        if not api_key:\n",
        "            raise ValueError(\"GEMINI_API_KEY environment variable is not set.\")\n",
        "\n",
        "        client = genai.Client(api_key=api_key)\n",
        "\n",
        "        response = client.models.generate_content(\n",
        "            model='gemini-2.5-flash',\n",
        "            contents=prompt         )\n",
        "\n",
        "        return \"Success\", response.text\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"LLM API Error/Missing Key: {e}\")\n",
        "        verdict = \"Bullish\" if change_30d > 0.5 else \"Bearish\" if change_30d < -0.5 else \"Neutral\"\n",
        "        fallback_advice = f\"**{verdict}** based on XGBoost (30-day forecast: {change_30d:.2f}%). LLM insight generation failed; check API key or connectivity.\"\n",
        "        return \"Caution\", fallback_advice\n",
        "\n",
        "# --- Streamlit App Layout ---\n",
        "st.title(\" Stock Price Forecasting App\")\n",
        "st.markdown(\"This app uses **XGBoost** models and **Gemini AI** to forecast and analyze stock price movements.\")\n",
        "\n",
        "# Sidebar only for inputs\n",
        "with st.sidebar:\n",
        "    st.header(\"Settings\")\n",
        "    ticker = st.text_input(\"Enter Stock Ticker (Yahoo Finance format):\", value=\"HDFCBANK.NS\")\n",
        "    st.caption(\"Examples: RELIANCE.NS, TCS.NS, INFY.NS\")\n",
        "    predict_btn = st.button(\"Generate Forecast\")\n",
        "\n",
        "    st.markdown(\"---\")\n",
        "    st.info(\"The Model is trained based on closing price for HDFC Bank from Jan 2020 - Nov 2025. Data Drift not taken into consideration due to short tenure.\")\n",
        "\n",
        "\n",
        "# Main Execution\n",
        "if predict_btn:\n",
        "    if not (xgb_1d and xgb_30d and scaler and FEATURE_IMPORTANCE):\n",
        "        st.stop()\n",
        "\n",
        "    with st.spinner(f\"Fetching data and calculating indicators for {ticker}...\"):\n",
        "        try:\n",
        "            # Fetch Data\n",
        "            end_date = datetime.today()\n",
        "            start_date = end_date - timedelta(days=5*365)\n",
        "            df = yf.download(ticker, start=start_date, end=end_date, progress=False, auto_adjust=True)\n",
        "\n",
        "            # --- CRITICAL FIX: Rename 'Close' to 'Adj Close' ---\n",
        "            if 'Close' in df.columns:\n",
        "                df.rename(columns={'Close': 'Adj Close'}, inplace=True)\n",
        "\n",
        "            # Data validation checks\n",
        "            if df.empty:\n",
        "                st.error(f\"No data found for {ticker}. Check the ticker symbol.\")\n",
        "            elif len(df) < 30:\n",
        "                st.error(f\"Only {len(df)} days of data were retrieved for {ticker}. Need more historical data (50+ days recommended) to calculate all indicators.\")\n",
        "\n",
        "            else:\n",
        "                last_features, last_price, df_processed = preprocess_new_data(df, scaler)\n",
        "\n",
        "                if last_features is not None:\n",
        "\n",
        "                    # Predict\n",
        "                    pred_ret_1d = xgb_1d.predict(last_features, validate_features=False)[0]\n",
        "                    pred_ret_30d = xgb_30d.predict(last_features, validate_features=False)[0]\n",
        "\n",
        "                    # Convert to Price\n",
        "                    pred_price_1d = last_price * np.exp(pred_ret_1d)\n",
        "                    pred_price_30d = last_price * np.exp(pred_ret_30d)\n",
        "\n",
        "                    # Calculate Change (Percentage)\n",
        "                    change_1d = (pred_price_1d - last_price) / last_price * 100\n",
        "                    change_30d = (pred_price_30d - last_price) / last_price * 100\n",
        "\n",
        "                    # --- Display AI Advice (LLM Feature) ---\n",
        "                    current_r2 = MODEL_METRICS['30D_R2']\n",
        "                    emoji, advice_text = generate_llm_insight(\n",
        "                        ticker,\n",
        "                        last_price,\n",
        "                        pred_price_1d,\n",
        "                        pred_price_30d,\n",
        "                        change_1d,\n",
        "                        change_30d,\n",
        "                        FEATURE_IMPORTANCE,\n",
        "                        current_r2\n",
        "                    )\n",
        "                    st.subheader(f\"Insights {emoji}\")\n",
        "                    st.markdown(f\"{advice_text}\")\n",
        "                    st.markdown(\"---\")\n",
        "\n",
        "                    # --- Display Clean Metrics ---\n",
        "                    st.subheader(f\"Forecast Results: {ticker}\")\n",
        "                    st.metric(label=f\"Last Close Price ({df_processed.index[-1].strftime('%Y-%m-%d')})\", value=f\"₹{last_price:.2f}\")\n",
        "\n",
        "                    col1, col2 = st.columns(2)\n",
        "\n",
        "                    col1.metric(\n",
        "                        label=\"1-Day Predicted Price\",\n",
        "                        value=f\"₹{pred_price_1d:.2f}\",\n",
        "                        delta=f\"{change_1d:.2f}%\",\n",
        "                        delta_color=\"normal\"\n",
        "                    )\n",
        "\n",
        "                    col2.metric(\n",
        "                        label=\"30-Day Predicted Price\",\n",
        "                        value=f\"₹{pred_price_30d:.2f}\",\n",
        "                        delta=f\"{change_30d:.2f}%\",\n",
        "                        delta_color=\"normal\"\n",
        "                    )\n",
        "\n",
        "                    st.markdown(\"---\")\n",
        "\n",
        "                    # --- Visualizations (Using Tabs) ---\n",
        "                    tab1, tab2, tab3 = st.tabs([\" Live Forecast & Trend\", \" Model Performance & History\", \" Data Analysis\"])\n",
        "\n",
        "                    with tab1: # Live Forecast & Trend\n",
        "                        st.subheader(\"Historical Price Trend & Live Forecast\")\n",
        "                        chart_data = df_processed['Adj Close'].tail(500)\n",
        "\n",
        "                        fig, ax = plt.subplots(figsize=(10, 5))\n",
        "                        ax.plot(chart_data.index, chart_data.values, label='Adj Close', color='#1f77b4', linewidth=2)\n",
        "\n",
        "                        ax.axhline(last_price, color='red', linestyle='--', alpha=0.8, label='Last Close')\n",
        "                        ax.axhline(pred_price_30d, color='green', linestyle=':', alpha=0.8, label='30-Day Target')\n",
        "\n",
        "                        ax.set_title(f\"{ticker} Adjusted Close Price (Last 500 Days)\", fontsize=14)\n",
        "                        ax.set_xlabel(\"Date\")\n",
        "                        ax.set_ylabel(\"Price (INR)\")\n",
        "                        ax.legend()\n",
        "                        ax.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
        "                        st.pyplot(fig)\n",
        "\n",
        "                    with tab2: # Model Performance & History (Data Drift Added Here)\n",
        "                        st.subheader(\"Model Accuracy and Health Check\")\n",
        "\n",
        "                        # R2 Metrics and Warning\n",
        "                        colA, colB = st.columns(2)\n",
        "                        colA.metric(\"1-Day Model R² (Test)\", f\"{MODEL_METRICS['1D_R2']:.2f}\")\n",
        "                        colB.metric(\"30-Day Model R² (Test)\", f\"{MODEL_METRICS['30D_R2']:.2f}\")\n",
        "\n",
        "                        if MODEL_METRICS['30D_R2'] < MODEL_METRICS['Retrain_Threshold']:\n",
        "                            st.warning(f\" **WARNING:** 30-Day Model R² is below the {MODEL_METRICS['Retrain_Threshold']:.2f} threshold. **Retraining is Recommended!**\")\n",
        "                        else:\n",
        "                            st.success(\" Model R² is healthy and above the retraining threshold.\")\n",
        "                        st.caption(\"R² is the coefficient of determination on the test data.\")\n",
        "\n",
        "                        st.markdown(\"---\")\n",
        "\n",
        "                        # --- START: Data Drift Comparison Chart ---\n",
        "                        st.subheader(\"Data Drift Check: Log Returns Distribution\")\n",
        "                        st.caption(\"Comparison of recent Log Returns vs. previous period to detect distribution shifts.\")\n",
        "\n",
        "                        # Define periods (90 trading days is approx. 4.5 months)\n",
        "                        DAYS_COMPARE = 90\n",
        "\n",
        "                        # Data extraction\n",
        "                        log_returns = df_processed['Log_Returns'].dropna()\n",
        "\n",
        "                        if len(log_returns) > DAYS_COMPARE * 2:\n",
        "                            # Split into two periods\n",
        "                            recent_returns = log_returns.iloc[-DAYS_COMPARE:]\n",
        "                            previous_returns = log_returns.iloc[-(DAYS_COMPARE * 2):-DAYS_COMPARE]\n",
        "\n",
        "                            fig_drift, ax_drift = plt.subplots(figsize=(10, 5))\n",
        "\n",
        "                            # Plot distributions\n",
        "                            sns.kdeplot(recent_returns, ax=ax_drift, label=f'Recent ({recent_returns.index[0].strftime(\"%Y-%m-%d\")} to Today)', color='red', fill=True, alpha=0.3)\n",
        "                            sns.kdeplot(previous_returns, ax=ax_drift, label=f'Previous ({previous_returns.index[0].strftime(\"%Y-%m-%d\")} to {previous_returns.index[-1].strftime(\"%Y-%m-%d\")})', color='blue', fill=True, alpha=0.3)\n",
        "\n",
        "                            ax_drift.set_title(f\"Data Drift: Log Returns Distribution Comparison\", fontsize=14)\n",
        "                            ax_drift.set_xlabel(\"Log Returns\")\n",
        "                            ax_drift.set_ylabel(\"Density (KDE)\")\n",
        "                            ax_drift.legend()\n",
        "                            ax_drift.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
        "                            st.pyplot(fig_drift)\n",
        "\n",
        "                        else:\n",
        "                            st.info(f\"Not enough data points available (need > {DAYS_COMPARE*2} points) to perform a meaningful drift comparison.\")\n",
        "\n",
        "                        st.markdown(\"---\")\n",
        "                        # --- END: Data Drift Comparison Chart ---\n",
        "\n",
        "                        # Historical Accuracy Chart\n",
        "                        if HISTORICAL_PREDICTIONS is not None:\n",
        "                            st.subheader(\"Actual vs. Predicted Price (Historical Test Data)\")\n",
        "\n",
        "                            df_plot = HISTORICAL_PREDICTIONS.tail(120)\n",
        "\n",
        "                            fig_comp, ax_comp = plt.subplots(figsize=(10, 5))\n",
        "\n",
        "                            ax_comp.plot(df_plot.index, df_plot['Actual_Price'], label='Actual Price', color='black', alpha=0.7)\n",
        "                            ax_comp.plot(df_plot.index, df_plot['Predicted_Price_1D'], label='1-Day Predicted', color='red', linestyle='--', alpha=0.7)\n",
        "                            ax_comp.plot(df_plot.index, df_plot['Predicted_Price_30D'], label='30-Day Predicted', color='green', linestyle=':', alpha=0.7)\n",
        "\n",
        "                            ax_comp.set_title(f\"Model Performance: Actual vs. Predicted Prices (Last 120 Historical Predictions)\", fontsize=14)\n",
        "                            ax_comp.set_xlabel(\"Date\")\n",
        "                            ax_comp.set_ylabel(\"Price (INR)\")\n",
        "                            ax_comp.legend()\n",
        "                            ax_comp.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
        "                            st.pyplot(fig_comp)\n",
        "                        else:\n",
        "                            st.info(\"Historical predictions data not available for plotting.\")\n",
        "\n",
        "                    with tab3: # Data Analysis\n",
        "                        st.subheader(\"Historical Log Returns Distribution\")\n",
        "                        fig_hist, ax_hist = plt.subplots(figsize=(10, 5))\n",
        "\n",
        "                        sns.histplot(df_processed['Log_Returns'].dropna(), bins=50, kde=True, ax=ax_hist, color='skyblue', edgecolor='black')\n",
        "\n",
        "                        ax_hist.axvline(df_processed['Log_Returns'].iloc[-1].item(), color='red', linestyle='--', label='Last Log Return')\n",
        "                        ax_hist.set_title(f\"{ticker} Daily Log Returns Distribution\", fontsize=14)\n",
        "                        ax_hist.set_xlabel(\"Log Returns\")\n",
        "                        ax_hist.set_ylabel(\"Frequency\")\n",
        "                        ax_hist.legend()\n",
        "                        ax_hist.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
        "                        st.pyplot(fig_hist)\n",
        "\n",
        "\n",
        "                else:\n",
        "                    st.error(\"Not enough data to calculate all required technical indicators (e.g., due to missing values).\")\n",
        "\n",
        "        except Exception as e:\n",
        "            st.error(f\"An error occurred: {e}\")\n",
        "\n",
        "else:\n",
        "    st.info(\"Enter a ticker symbol and click 'Generate Forecast' to begin.\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing app.py\n"
          ]
        }
      ],
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yUGwPW0Mz3tc",
        "outputId": "b0c0875b-3a96-48f8-a415-b06cea9cf176"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- MANDATORY IMPORTS AND SETUP FOR EXECUTION ---\n",
        "import os\n",
        "import subprocess\n",
        "import time\n",
        "from pyngrok import ngrok\n",
        "from pyngrok.exception import PyngrokNgrokError\n",
        "\n",
        "# 2. SET ENVIRONMENT VARIABLES (Crucial)\n",
        "os.environ[\"GEMINI_API_KEY\"] = \"AIzaSyB7wM_txavE_2xLsHQvs7fqwwKfO2iiDko\"\n",
        "os.environ[\"NGROK_AUTH_TOKEN\"] = \"360zPaErQ3n0rJnh34kIqFHFF3N_3PuofpKeMEuGZYLe8NJpd\"\n",
        "# ----------------------------------------------------\n",
        "\n",
        "print(\"\\n--- Starting Streamlit App Deployment ---\")\n",
        "\n",
        "# 1. Cleanup previous processes\n",
        "print(\"Stopping any existing Streamlit and ngrok processes...\")\n",
        "# Use '2> /dev/null' to suppress error messages if no process is found\n",
        "!kill $(lsof -t -i:8501) 2> /dev/null\n",
        "!killall ngrok 2> /dev/null\n",
        "time.sleep(2)\n",
        "\n",
        "# 2. Set ngrok auth token via function (most robust way)\n",
        "NGROK_AUTH_TOKEN = os.environ.get(\"NGROK_AUTH_TOKEN\")\n",
        "if NGROK_AUTH_TOKEN:\n",
        "    ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n",
        "else:\n",
        "    print(\"Warning: NGROK_AUTH_TOKEN not found. Ngrok may fail.\")\n",
        "\n",
        "# 3. Start Streamlit in the background\n",
        "print(\"Starting Streamlit server on port 8501...\")\n",
        "# Capture stdout and stderr for better crash detection\n",
        "process = subprocess.Popen(['streamlit', 'run', 'app.py', '--server.port', '8501', '--server.headless', 'true'],\n",
        "                           stdout=subprocess.PIPE,\n",
        "                           stderr=subprocess.PIPE)\n",
        "time.sleep(5) # Give Streamlit time to start up\n",
        "\n",
        "# CRITICAL CHECK: Check if Streamlit failed to start after 5 seconds\n",
        "if process.poll() is not None:\n",
        "    print(\"\\n--- CRITICAL STREAMLIT CRASH DETECTED ---\")\n",
        "\n",
        "    # Read and decode the error log from the standard error stream\n",
        "    stderr_output = process.stderr.read().decode()\n",
        "    if stderr_output:\n",
        "        print(\"STREAMLIT ERROR LOG (Traceback below):\")\n",
        "        print(stderr_output)\n",
        "        print(\"\\n**Action required: Check if all model files (.pkl, .csv) are uploaded to your Colab root directory and the app.py file exists.**\")\n",
        "    else:\n",
        "        print(\"Streamlit process terminated unexpectedly without a visible error log.\")\n",
        "    print(\"-------------------------------------------\\n\")\n",
        "else:\n",
        "    # 4. Create the ngrok tunnel ONLY if Streamlit is running\n",
        "    public_url = None\n",
        "    try:\n",
        "        ngrok_tunnel = ngrok.connect(8501)\n",
        "        public_url = ngrok_tunnel.public_url\n",
        "\n",
        "    except PyngrokNgrokError as e:\n",
        "        print(f\"\\n--- ERROR ---\")\n",
        "        print(f\"Ngrok connection failed. Check your ngrok setup and token.\")\n",
        "        print(f\"Error details: {e}\")\n",
        "        public_url = \"http://localhost:8501\"\n",
        "\n",
        "    if public_url:\n",
        "        print(\"\\n=======================================================\")\n",
        "        print(\"  Streamlit App is LIVE! Click the URL below:\")\n",
        "        print(f\"  {public_url}\")\n",
        "        print(\"=======================================================\\n\")\n",
        "\n",
        "        # 5. Keep the cell running to maintain the ngrok tunnel\n",
        "        # Press the 'stop' button (square icon) in the notebook cell to terminate\n",
        "        try:\n",
        "            # Use a simple infinite loop to keep the execution context alive\n",
        "            while True:\n",
        "                time.sleep(1)\n",
        "        except KeyboardInterrupt:\n",
        "            print(\"\\n[Ctrl+C detected] Terminating processes...\")\n",
        "        finally:\n",
        "            # Clean up processes on termination\n",
        "            if process.poll() is None:\n",
        "                process.terminate()\n",
        "            ngrok.kill()\n",
        "            print(\"Streamlit server and ngrok tunnel terminated cleanly.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LiXCgBaA44ed",
        "outputId": "d563a201-b164-490f-e574-1be5fb2208c5"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Starting Streamlit App Deployment ---\n",
            "Stopping any existing Streamlit and ngrok processes...\n",
            "Starting Streamlit server on port 8501...\n",
            "\n",
            "=======================================================\n",
            "  Streamlit App is LIVE! Click the URL below:\n",
            "  https://avellan-kristine-finer.ngrok-free.dev\n",
            "=======================================================\n",
            "\n",
            "\n",
            "[Ctrl+C detected] Terminating processes...\n",
            "Streamlit server and ngrok tunnel terminated cleanly.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}